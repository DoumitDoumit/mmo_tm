{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import cvxpy as cp\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from src.shortest_paths_gt import get_graph_props\n",
    "from src.load_data import (\n",
    "    read_metadata_networks_tntp,\n",
    "    read_graph_transport_networks_tntp,\n",
    "    read_traffic_mat_transport_networks_tntp,\n",
    ") \n",
    "from src.models import SDModel, BeckmannModel, TwostageModel\n",
    "from src.algs import subgd, ustm, frank_wolfe, cyclic\n",
    "from src.cvxpy_solvers import get_max_traffic_mat_mul\n",
    "from src.commons import Correspondences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 14})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "%matplotlib inline\n",
    "networks_path = Path(\"./TransportationNetworks\")\n",
    "\n",
    "folder = \"SiouxFalls\"\n",
    "net_name = \"SiouxFalls_net\"\n",
    "traffic_mat_name = \"SiouxFalls_trips\"\n",
    "\n",
    "net_file = networks_path / folder / f\"{net_name}.tntp\"\n",
    "traffic_mat_file = networks_path / folder / f\"{traffic_mat_name}.tntp\"\n",
    "graph, metadata = read_graph_transport_networks_tntp(net_file)\n",
    "correspondences = read_traffic_mat_transport_networks_tntp(traffic_mat_file, metadata)\n",
    "n = graph.number_of_nodes()\n",
    "\n",
    "print(f\"{graph.number_of_edges()=}, {graph.number_of_nodes()=}\")\n",
    "beckmann_model = BeckmannModel(graph, correspondences)\n",
    "\n",
    "eps = 1e-4\n",
    "mean_bw = beckmann_model.graph.ep.capacities.a.mean()\n",
    "mean_cost = beckmann_model.graph.ep.free_flow_times.a.mean()\n",
    "\n",
    "\n",
    "eps_abs = eps * mean_cost * mean_bw * graph.number_of_edges()\n",
    "\n",
    "eps_cons_abs = eps * mean_bw \n",
    "\n",
    "# sum of capacity violation <= eps * average link capacity\n",
    "print(eps_abs, eps_cons_abs)\n",
    "\n",
    "# Compare with Frank-Wolfe method\n",
    "#times_e_fw, flows_e_fw, logs, optimal = frank_wolfe(beckmann_model, eps_abs)\n",
    "times_e_fw, flows_e_fw, _, _ = frank_wolfe(beckmann_model, eps_abs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def S(e,A,y):\n",
    "    product = -np.dot(A.T,y)[e]\n",
    "#     print(product,e)\n",
    "    max_ = np.max(product)\n",
    "    #print(max_.shape)\n",
    "    i_star = np.argwhere(product==max_)[0][0]\n",
    "    return np.array([max_, i_star])\n",
    "def gradient_sigma_star_e(s,e,t,f,rho,mu):\n",
    "    fe = f[e]\n",
    "    te = t[e]\n",
    "#     print(f\"{s,e,te,fe,rho,mu=}\")\n",
    "    п = max(s, te)\n",
    "    result = fe*(((п-te)/(te*rho))**mu)  * (s - te > 0)\n",
    "#     s,e,te,fe,rho,mu=(-0.0, 0, 6.0, 25900.201171875, 0.15000000596046448, 0.25)\n",
    "    return result\n",
    "def calculate_M(e,A,y):\n",
    "    M = np.zeros_like(y)\n",
    "    i_star = S(e,A,y)[1]\n",
    "    ## - A_me * delta_i*n = M_mn\n",
    "    for m in range(M.shape[0]):\n",
    "        for n in range(M.shape[1]):\n",
    "            if n == i_star:\n",
    "                delta = 1\n",
    "            else:\n",
    "                delta = 0\n",
    "            M[m][n] = - A[m][e] * delta\n",
    "    return M\n",
    "\n",
    "def gradient_fy(y,A,t,f,rho,mu,L):\n",
    "    sum_ = 0\n",
    "    for e in range(A.shape[0]):\n",
    "        s = S(e,A,y)[0]\n",
    "        sigma_ = gradient_sigma_star_e(s,e,t,f,rho,mu)\n",
    "        Me = calculate_M(e,A,y)\n",
    "        product = sigma_ * Me\n",
    "        sum_ += product\n",
    "#         print(f\"{e, sum_ = }\")\n",
    "    sum_ -= L\n",
    "    return sum_\n",
    "\n",
    "def grady(y):\n",
    "    return gradient_fy(y,incidence_matrix,fft,caps,rho,mu,L)\n",
    "# def compute_flow_values(optimized_y, incidence_matrix):\n",
    "#     flow_values = np.dot(incidence_matrix.T, optimized_y).sum(axis=1)\n",
    "#     return flow_values\n",
    "def compute_flow_values(optimized_y, incidence_matrix):\n",
    "    z = np.min(incidence_matrix.T @ optimized_y, axis=1)\n",
    "    amin = np.argmin(incidence_matrix.T @ optimized_y, axis=1)\n",
    "    \n",
    "    flow_values = np.zeros((incidence_matrix.T.shape))\n",
    "#     amin_ind_tuples = np.vstack((np.arange(flow_values.shape[0]), amin)).T\n",
    "    flow_values[np.arange(flow_values.shape[0]), amin] = beckmann_model.tau_inv(np.maximum(fft, -z))\n",
    "    \n",
    "#     flow_values[*amin_ind_tuples] = beckmann_model.tau_inv(np.maximum(fft, -z))\n",
    "    assert np.all(np.isfinite(flow_values))\n",
    "    return flow_values\n",
    "\n",
    "def gradient_descent(y_init, learning_rate, num_iterations, fw_results):\n",
    "    y = y_init\n",
    "    flow_values_sum = np.zeros(incidence_matrix.T.shape)\n",
    "    cons_log = []\n",
    "    for i in range(num_iterations):\n",
    "        flow_values = compute_flow_values(y, incidence_matrix)\n",
    "        grad = incidence_matrix @ flow_values + L.T\n",
    "        y += learning_rate * grad\n",
    "        flow_values_sum += flow_values\n",
    "\n",
    "        if not i % 1000:  # Update the plot with both FW and GD results\n",
    "            plt.plot(fw_results, label=\"FW\")  # Use orange color for FW results\n",
    "            plt.plot(flow_values_sum.sum(axis=1) / (i + 1), label=\"GD\")  # Use blue color for GD results\n",
    "            plt.xlabel('Edge index')  # Change x-axis label to 'Edge index'\n",
    "            plt.ylabel('Flow on edge')\n",
    "            plt.legend(), plt.show()\n",
    "\n",
    "        cons_log.append(np.linalg.norm(incidence_matrix @ flow_values_sum / (i + 1) + L.T))\n",
    "\n",
    "    flow_values_avg = flow_values_sum / num_iterations\n",
    "    return y, flow_values_avg, cons_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def softmin(x, beta):\n",
    "#     \"\"\" Compute the smooth minimum of an array x with smoothing parameter beta \"\"\"\n",
    "# #     max_x = np.max(x)\n",
    "# #     x_stable = x - max_x\n",
    "# #     weights = np.exp(-beta * x_stable)\n",
    "# #     return np.dot(weights, x) / np.sum(weights)\n",
    "#     return -logsumexp(-beta * x) / beta\n",
    "# #     return -logsumexp(-x)\n",
    "\n",
    "# # # Function to compute sigma_star, which is the sum of squares of the elements\n",
    "# # def sigma_star(t):\n",
    "# #     \"\"\" Compute the sum of squares of the elements of the array t \"\"\"\n",
    "# #     return (t ** 2).sum()\n",
    "\n",
    "def numerical_gradient(f, x, eps=1e-8):\n",
    "    # Set up an array to store our gradient calculations.\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    # Let's go through each element in x to find the gradients.\n",
    "    for i in range(len(x)):\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += eps  # Nudge the element up a bit by eps.\n",
    "        \n",
    "        x_minus = x.copy()\n",
    "        x_minus[i] -= eps  # Now nudge it down by eps and see what happens.\n",
    "        \n",
    "        # Calculate the gradient using the central difference formula.\n",
    "        grad[i] = (f(x_plus) - f(x_minus)) / (2 * eps)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "def softmin(x, beta):\n",
    "    \"\"\" Compute the smooth minimum of an array x with smoothing parameter beta \"\"\"\n",
    "    return -logsumexp(-beta * x) / beta\n",
    "\n",
    "\n",
    "# Define the dual objective function using the softmin\n",
    "def dual_objective(y_flattened, A, L, beta):\n",
    "    \"\"\" The dual objective function to be minimized \"\"\"\n",
    "    y = y_flattened.reshape((nodes, nodes))\n",
    "    ATy = A.T @ y\n",
    "    \n",
    "    # Apply softmin to each line of ATy\n",
    "    softmin_values = np.array([softmin(ATy_row, beta) for ATy_row in ATy])\n",
    "    \n",
    "    # Calculate the value of the target function\n",
    "    objective_value = np.sum(beckmann_model.sigma_star(-softmin_values)) - np.sum(y * L)\n",
    "    \n",
    "    if np.isnan(objective_value) or np.isinf(objective_value):\n",
    "        print(\"NaN or Inf detected in objective function!\")\n",
    "        return np.finfo(float).max\n",
    "    \n",
    "    return objective_value\n",
    "\n",
    "def dual_objective_grad(y_flattened, A, L, beta):\n",
    "    \"\"\" Gradient of the target function of the dual problem \"\"\"\n",
    "    y = y_flattened.reshape((nodes, nodes))\n",
    "    ATy = A.T @ y\n",
    "    \n",
    "    # Calculate softmin for each line of ATy\n",
    "    softmin_values = np.array([softmin(ATy_row, beta) for ATy_row in ATy])\n",
    "    \n",
    "    # Calculate the derivative of softmin\n",
    "    softmin_grad = np.exp(-beta * (ATy - softmin_values[:, np.newaxis]))\n",
    "    softmin_grad /= np.sum(softmin_grad, axis=1, keepdims=True)\n",
    "    \n",
    "    # Calculate the sigma_star gradient numerically\n",
    "    sigma_star_grad = numerical_gradient(lambda x: np.sum(beckmann_model.sigma_star(x)), -softmin_values)\n",
    "    \n",
    "    # Calculate the y gradient\n",
    "    grad = -A @ (sigma_star_grad[:, np.newaxis] * softmin_grad) - L\n",
    "    \n",
    "    grad_flattened = grad.flatten()\n",
    "    \n",
    "    if np.isnan(grad_flattened).any() or np.isinf(grad_flattened).any():\n",
    "        print(\"NaN or Inf detected in gradient!\")\n",
    "        return np.zeros_like(grad_flattened)\n",
    "    \n",
    "    return grad_flattened\n",
    "\n",
    "def scaled_minimize(fun, x0, args=(), jac=None, **kwargs):\n",
    "    # Determine the scale based on the maximum absolute value of initial guesses to prevent numerical issues\n",
    "    scale = np.max(np.abs(x0)) + 1e-8\n",
    "    \n",
    "    # Define a scaled version of the original function to handle scale differences in variables\n",
    "    def scaled_fun(x, *args):\n",
    "        return fun(x * scale, *args)\n",
    "    \n",
    "    # If a Jacobian function is provided, scale it appropriately to correspond with the scaled function\n",
    "    def scaled_jac(x, *args):\n",
    "        return jac(x * scale, *args) * scale\n",
    "    \n",
    "    # Perform minimization on the scaled function and adjust the initial guess and Jacobian accordingly\n",
    "    result = minimize(scaled_fun, x0 / scale, args=args, jac=scaled_jac if jac else None, **kwargs)\n",
    "    \n",
    "    # Adjust the result to reflect the original scale and return the scaled optimization result\n",
    "    return OptimizeResult(\n",
    "        x=result.x * scale,  # Scale the solution back to the original scale\n",
    "        fun=result.fun,  # The function value at the solution\n",
    "        jac=result.jac / scale if result.jac is not None else None,  # Scale the Jacobian back if it exists\n",
    "        **{k: v for k, v in result.items() if k not in ['x', 'fun', 'jac']}  # Pass through other info unchanged\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incidence_matrix = nx.incidence_matrix(graph, oriented=True).todense()\n",
    "correspondence_matrix = np.array(correspondences.traffic_mat)\n",
    "L = np.diag(correspondence_matrix.sum(axis=1)) - correspondence_matrix\n",
    "\n",
    "print(L.shape)\n",
    "print(incidence_matrix.shape)\n",
    "n_ = incidence_matrix.shape[0]\n",
    "m_ = incidence_matrix.shape[1]\n",
    "\n",
    "# Initializing parameters for the method\n",
    "y_init = np.zeros((n_, n_))\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "fft, mu, rho, caps = get_graph_props(beckmann_model.graph)\n",
    "\n",
    "mu = mu[0]\n",
    "rho = rho[0]\n",
    "\n",
    "-np.dot(incidence_matrix.T,y_init)[0]\n",
    "\n",
    "s,e,te,fe,rho,mu=(-0.0, 0, 6.0, 25900.201171875, 0.15000000596046448, 0.25)\n",
    "\n",
    "fe*(((s-te)/(te*rho))**mu)\n",
    "((s-te)/(te*rho)) ** mu\n",
    "\n",
    "incidence_matrix[:, 0]\n",
    "\n",
    "x = np.arange(25).reshape((5,5)) \n",
    "# x[*[(0,1),(1,0)]] = np.array([100, 200])\n",
    "x[np.arange(5), np.array([0, 1, 0, 1, 3])] = 1000\n",
    "x\n",
    "\n",
    "results = gradient_descent(y_init=np.zeros((n_, n_)), learning_rate=0.00001, num_iterations=100000, fw_results=flows_e_fw)\n",
    "y_final, flow_values_avg, cons_log = results\n",
    "\n",
    "y = results[0]\n",
    "\n",
    "z = np.min(incidence_matrix.T @ y, axis=1)\n",
    "amin = np.argmin(incidence_matrix.T @ y, axis=1)\n",
    "\n",
    "flow_values = np.zeros((incidence_matrix.T.shape))\n",
    "# amin_ind_tuples = np.vstack((np.arange(flow_values.shape[0]), amin)).T\n",
    "# amin_ind_tuples\n",
    "# flow_values[*amin_ind_tuples]\n",
    "flow_values[np.arange(flow_values.shape[0]), amin] = 1\n",
    "flow_values\n",
    "\n",
    "cons_log = results[-1] \n",
    "plt.plot(cons_log)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Constraint violation')\n",
    "plt.show()\n",
    "\n",
    "# Create comparing GD & FW\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(flows_e_fw, label=\"Frank-Wolfe\", color='orange', marker='o')\n",
    "plt.plot(flow_values_avg.sum(axis=1), label=\"Gradient Descent\", color='blue', marker='x')\n",
    "plt.xlabel('Edge index')\n",
    "plt.ylabel('Flow on edge')\n",
    "plt.title('Comparison of FW and GD Flow Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
