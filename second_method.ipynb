{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "from scipy.special import logsumexp\n",
    "from scipy.optimize import minimize, OptimizeResult\n",
    "\n",
    "# Import functions for loading TNTP data\n",
    "from src.load_data import (\n",
    "    read_graph_transport_networks_tntp,\n",
    "    read_traffic_mat_transport_networks_tntp,\n",
    ")\n",
    "\n",
    "from src.models import BeckmannModel"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def create_or_load_traffic_model():\n",
    "    # Start by creating a directed graph. I'll use this to model our traffic routes.\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Adding edges between nodes. The weights might represent something like road capacity or a default cost.\n",
    "    G.add_edge(1, 2, weight=1)  # Direct route from node 1 to node 2.\n",
    "    G.add_edge(2, 3, weight=1)  # Another direct route, this time from node 2 to node 3.\n",
    "    G.add_edge(3, 1, weight=1)  # And a route that loops back from node 3 to node 1.\n",
    "\n",
    "    # The graph is all set up now, so let's return it.\n",
    "    return G\n",
    "\n",
    "# I need the traffic model graph for further analysis, so I'll call the function I just defined.\n",
    "traffic_model_nx_graph = create_or_load_traffic_model()\n",
    "\n",
    "# Now, let's create an incidence matrix from this graph.\n",
    "# The incidence matrix is quite handy to describe relationships in the graph—rows for nodes and columns for edges.\n",
    "incidence_mat = nx.incidence_matrix(traffic_model_nx_graph, oriented=True).todense()  # Make it a dense matrix for easier manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.array([1, 2, 2, 2])\n",
    "# softmin(x, beta=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def softmin(x, beta):\n",
    "#     \"\"\" Compute the smooth minimum of an array x with smoothing parameter beta \"\"\"\n",
    "# #     max_x = np.max(x)\n",
    "# #     x_stable = x - max_x\n",
    "# #     weights = np.exp(-beta * x_stable)\n",
    "# #     return np.dot(weights, x) / np.sum(weights)\n",
    "#     return -logsumexp(-beta * x) / beta\n",
    "# #     return -logsumexp(-x)\n",
    "\n",
    "# # # Function to compute sigma_star, which is the sum of squares of the elements\n",
    "# # def sigma_star(t):\n",
    "# #     \"\"\" Compute the sum of squares of the elements of the array t \"\"\"\n",
    "# #     return (t ** 2).sum()\n",
    "\n",
    "def numerical_gradient(f, x, eps=1e-8):\n",
    "    # Set up an array to store our gradient calculations.\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    # Let's go through each element in x to find the gradients.\n",
    "    for i in range(len(x)):\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += eps  # Nudge the element up a bit by eps.\n",
    "        \n",
    "        x_minus = x.copy()\n",
    "        x_minus[i] -= eps  # Now nudge it down by eps and see what happens.\n",
    "        \n",
    "        # Calculate the gradient using the central difference formula.\n",
    "        grad[i] = (f(x_plus) - f(x_minus)) / (2 * eps)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "def softmin(x, beta):\n",
    "    \"\"\" Compute the smooth minimum of an array x with smoothing parameter beta \"\"\"\n",
    "    return -logsumexp(-beta * x) / beta\n",
    "\n",
    "\n",
    "# Define the dual objective function using the softmin\n",
    "def dual_objective(y_flattened, A, L, beta):\n",
    "    \"\"\" The dual objective function to be minimized \"\"\"\n",
    "    y = y_flattened.reshape((nodes, nodes))\n",
    "    ATy = A.T @ y\n",
    "    \n",
    "    # Применяем softmin к каждой строке ATy\n",
    "    softmin_values = np.array([softmin(ATy_row, beta) for ATy_row in ATy])\n",
    "    \n",
    "    # Вычисляем значение целевой функции\n",
    "    objective_value = np.sum(beckmann_model.sigma_star(-softmin_values)) - np.sum(y * L)\n",
    "    \n",
    "    if np.isnan(objective_value) or np.isinf(objective_value):\n",
    "        print(\"NaN or Inf detected in objective function!\")\n",
    "        return np.finfo(float).max\n",
    "    \n",
    "    return objective_value\n",
    "\n",
    "def dual_objective_grad(y_flattened, A, L, beta):\n",
    "    \"\"\" Gradient of the target function of the dual problem \"\"\"\n",
    "    y = y_flattened.reshape((nodes, nodes))\n",
    "    ATy = A.T @ y\n",
    "    \n",
    "    # Calculate softmin for each line of ATy\n",
    "    softmin_values = np.array([softmin(ATy_row, beta) for ATy_row in ATy])\n",
    "    \n",
    "    # Calculate the derivative of softmin\n",
    "    softmin_grad = np.exp(-beta * (ATy - softmin_values[:, np.newaxis]))\n",
    "    softmin_grad /= np.sum(softmin_grad, axis=1, keepdims=True)\n",
    "    \n",
    "    # Calculate the sigma_star gradient numerically\n",
    "    sigma_star_grad = numerical_gradient(lambda x: np.sum(beckmann_model.sigma_star(x)), -softmin_values)\n",
    "    \n",
    "    # Calculate the y gradient\n",
    "    grad = -A @ (sigma_star_grad[:, np.newaxis] * softmin_grad) - L\n",
    "    \n",
    "    grad_flattened = grad.flatten()\n",
    "    \n",
    "    if np.isnan(grad_flattened).any() or np.isinf(grad_flattened).any():\n",
    "        print(\"NaN or Inf detected in gradient!\")\n",
    "        return np.zeros_like(grad_flattened)\n",
    "    \n",
    "    return grad_flattened\n",
    "\n",
    "def scaled_minimize(fun, x0, args=(), jac=None, **kwargs):\n",
    "    # Determine the scale based on the maximum absolute value of initial guesses to prevent numerical issues\n",
    "    scale = np.max(np.abs(x0)) + 1e-8\n",
    "    \n",
    "    # Define a scaled version of the original function to handle scale differences in variables\n",
    "    def scaled_fun(x, *args):\n",
    "        return fun(x * scale, *args)\n",
    "    \n",
    "    # If a Jacobian function is provided, scale it appropriately to correspond with the scaled function\n",
    "    def scaled_jac(x, *args):\n",
    "        return jac(x * scale, *args) * scale\n",
    "    \n",
    "    # Perform minimization on the scaled function and adjust the initial guess and Jacobian accordingly\n",
    "    result = minimize(scaled_fun, x0 / scale, args=args, jac=scaled_jac if jac else None, **kwargs)\n",
    "    \n",
    "    # Adjust the result to reflect the original scale and return the scaled optimization result\n",
    "    return OptimizeResult(\n",
    "        x=result.x * scale,  # Scale the solution back to the original scale\n",
    "        fun=result.fun,  # The function value at the solution\n",
    "        jac=result.jac / scale if result.jac is not None else None,  # Scale the Jacobian back if it exists\n",
    "        **{k: v for k, v in result.items() if k not in ['x', 'fun', 'jac']}  # Pass through other info unchanged\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata[\"can_pass_through_zones\"]=True\n"
     ]
    }
   ],
   "source": [
    "# Setting up the path to the TNTP data\n",
    "networks_path = Path(\"./TransportationNetworks\")\n",
    "folder = \"SiouxFalls\"\n",
    "net_name = \"SiouxFalls_net\"\n",
    "traffic_mat_name = \"SiouxFalls_trips\"\n",
    "\n",
    "# Loading graph and traffic data\n",
    "net_file = networks_path / folder / f\"{net_name}.tntp\"\n",
    "traffic_mat_file = networks_path / folder / f\"{traffic_mat_name}.tntp\"\n",
    "graph, metadata = read_graph_transport_networks_tntp(net_file)\n",
    "correspondences = read_traffic_mat_transport_networks_tntp(traffic_mat_file, metadata)\n",
    "\n",
    "beckmann_model = BeckmannModel(graph, correspondences)\n",
    "\n",
    "# Retrieving the number of nodes and edges from the graph\n",
    "nodes = graph.number_of_nodes()\n",
    "edges = graph.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incidence matrix (A) shape: (24, 76)\n",
      "Incidence matrix (A) sample:\n",
      " [[-1. -1.  1.  0.  1.]\n",
      " [ 1.  0. -1. -1.  0.]\n",
      " [ 0.  1.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "Demand matrix (L) shape: (24, 24)\n",
      "Demand matrix (L) sample:\n",
      " [[ 8800.  -100.  -100.  -500.  -200.]\n",
      " [ -100.  4000.  -100.  -200.  -100.]\n",
      " [ -100.  -100.  2800.  -200.  -100.]\n",
      " [ -500.  -200.  -200. 11600.  -500.]\n",
      " [ -200.  -100.  -100.  -500.  6100.]]\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          576     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.68300D+07    |proj g|=  4.51997D+04\n",
      "\n",
      "At iterate    1    f=  4.51569D+07    |proj g|=  3.17174D+04\n",
      "\n",
      "At iterate    2    f=  4.34603D+07    |proj g|=  5.92010D+04\n",
      "\n",
      "At iterate    3    f=  4.17771D+07    |proj g|=  9.54504D+03\n",
      "\n",
      "At iterate    4    f=  4.15237D+07    |proj g|=  4.55104D+03\n",
      "\n",
      "At iterate    5    f=  4.08495D+07    |proj g|=  9.07431D+03\n",
      "\n",
      "At iterate    6    f=  4.06865D+07    |proj g|=  1.07193D+04\n",
      "\n",
      "At iterate    7    f=  4.05354D+07    |proj g|=  3.89082D+03\n",
      "\n",
      "At iterate    8    f=  4.03838D+07    |proj g|=  3.59176D+03\n",
      "\n",
      "At iterate    9    f=  4.02978D+07    |proj g|=  5.63168D+03\n",
      "\n",
      "At iterate   10    f=  4.02176D+07    |proj g|=  3.04592D+03\n",
      "\n",
      "At iterate   11    f=  4.01634D+07    |proj g|=  2.02933D+03\n",
      "\n",
      "At iterate   12    f=  4.01310D+07    |proj g|=  1.66032D+03\n",
      "\n",
      "At iterate   13    f=  4.01083D+07    |proj g|=  1.40457D+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   14    f=  4.00937D+07    |proj g|=  2.71689D+03\n",
      "\n",
      "At iterate   15    f=  4.00823D+07    |proj g|=  1.07990D+03\n",
      "\n",
      "At iterate   16    f=  4.00784D+07    |proj g|=  4.74352D+02\n",
      "\n",
      "At iterate   17    f=  4.00729D+07    |proj g|=  3.12839D+02\n",
      "\n",
      "At iterate   18    f=  4.00698D+07    |proj g|=  1.49431D+03\n",
      "\n",
      "At iterate   19    f=  4.00672D+07    |proj g|=  3.49205D+02\n",
      "\n",
      "At iterate   20    f=  4.00662D+07    |proj g|=  1.45281D+02\n",
      "\n",
      "At iterate   21    f=  4.00655D+07    |proj g|=  4.29335D+02\n",
      "\n",
      "At iterate   22    f=  4.00652D+07    |proj g|=  7.30396D+02\n",
      "\n",
      "At iterate   23    f=  4.00647D+07    |proj g|=  9.69463D+01\n",
      "\n",
      "At iterate   24    f=  4.00645D+07    |proj g|=  9.36916D+01\n",
      "\n",
      "At iterate   25    f=  4.00643D+07    |proj g|=  1.60772D+02\n",
      "\n",
      "At iterate   26    f=  4.00642D+07    |proj g|=  2.96765D+02\n",
      "\n",
      "At iterate   27    f=  4.00641D+07    |proj g|=  1.38809D+02\n",
      "\n",
      "At iterate   28    f=  4.00640D+07    |proj g|=  2.80719D+01\n",
      "Optimization Result:   message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 40063992.31594628\n",
      "        x: [ 3.294e+01  1.394e+01 ...  2.132e+01  2.909e+01]\n",
      "      nit: 29\n",
      "      jac: [ 1.263e+01 -5.240e+00 ... -2.161e+00 -1.260e+01]\n",
      "     nfev: 33\n",
      "     njev: 33\n",
      " hess_inv: <576x576 LbfgsInvHessProduct with dtype=float64>\n",
      "Optimization Result:   message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 40063992.31594628\n",
      "        x: [ 3.294e+01  1.394e+01 ...  2.132e+01  2.909e+01]\n",
      "      nit: 29\n",
      "      jac: [ 1.263e+01 -5.240e+00 ... -2.161e+00 -1.260e+01]\n",
      "     nfev: 33\n",
      "     njev: 33\n",
      " hess_inv: <576x576 LbfgsInvHessProduct with dtype=float64>\n",
      "\n",
      "At iterate   29    f=  4.00640D+07    |proj g|=  4.45149D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  576     29     33      1     0     0   4.451D+01   4.006D+07\n",
      "  F =   40063992.315946281     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    }
   ],
   "source": [
    "# Generate the incidence matrix for the city model\n",
    "A = nx.incidence_matrix(graph, oriented=True).todense()\n",
    "print(\"Incidence matrix (A) shape:\", A.shape)\n",
    "print(\"Incidence matrix (A) sample:\\n\", A[:5, :5])  # First 5 rows and columns\n",
    "\n",
    "# Using the traffic matrix as the demand matrix L\n",
    "correspondence_matrix = np.array(correspondences.traffic_mat)  # Ensure it's in NumPy array form if not already\n",
    "L = np.diag(correspondence_matrix.sum(axis=1)) - correspondence_matrix.T\n",
    "\n",
    "print(\"Demand matrix (L) shape:\", L.shape)\n",
    "print(\"Demand matrix (L) sample:\\n\", L[:5, :5])  # First 5 rows and columns\n",
    "\n",
    "beta = 0.1\n",
    "y_init = np.random.rand(nodes, nodes).flatten() * 1e-3\n",
    "\n",
    "options = {\n",
    "    'maxiter': 1000,\n",
    "    'disp': True,\n",
    "    'ftol': 1e-6,\n",
    "    'gtol': 1e-6\n",
    "}\n",
    "\n",
    "# Perform the optimization using L-BFGS-B\n",
    "try:\n",
    "    result = minimize(dual_objective, y_init, jac=dual_objective_grad, args=(A, L, beta), method='L-BFGS-B', options=options)\n",
    "    print(\"Optimization Result:\", result)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during optimization: {e}\")\n",
    "\n",
    "print(\"Optimization Result:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tm",
   "language": "python",
   "name": "tm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
